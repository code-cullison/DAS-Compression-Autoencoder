{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/code-cullison/DAS-Compression-Autoencoder/blob/main/notebooks/DAS_Compression_Autoencoder.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "EKHEk9uhGWih"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Wy1XUbgScu"
      },
      "source": [
        "# DAS Compression using Autoencoder\n",
        "\n",
        "#### (Note: the notebook run time can take about an hour.)\n",
        "\n",
        "#### Authors:\n",
        "**Haipeng Li**\n",
        "<br>\n",
        "**Thomas Cullison**\n",
        "<br>\n",
        "**Hassan Almomin**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT88QaK2UQqt"
      },
      "source": [
        "# DAS Data Compression using Autoencoder\n",
        "\n",
        "This notebook demonstraights the use of an autoencoder model for lossy DAS data compression.  The model code we started from can be found here (we had to make modification for our purposes): [TensorFlow Compression](https://github.com/tensorflow/compression)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlHR0nhGVaXe"
      },
      "source": [
        "## Paper and posts for understanding related theories\n",
        "\n",
        "1. [End-to-end Optimized Image Compression](https://arxiv.org/abs/1611.01704)\n",
        "2. [Image Compression Using Autoencoders in Keras](https://blog.paperspace.com/autoencoder-image-compression-keras/)\n",
        "3. [AI-Based Image Compression: The State of the Art](https://towardsdatascience.com/ai-based-image-compression-the-state-of-the-art-fb5aa6042bfa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ni-kHP6hgQO"
      },
      "source": [
        "### Install Tensorflow Compression via `pip`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K489KsEgxuLI"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Installs the latest version of TFC compatible with the installed TF version.\n",
        "\n",
        "read MAJOR MINOR <<< \"$(pip show tensorflow | perl -p -0777 -e 's/.*Version: (\\d+)\\.(\\d+).*/\\1 \\2/sg')\"\n",
        "pip install \"tensorflow-compression<$MAJOR.$(($MINOR+1))\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0g4HSqUhrrG"
      },
      "source": [
        "### Install numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfulAw58AmVW"
      },
      "outputs": [],
      "source": [
        "!python -m pip install numba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfVAmHCVxpTS"
      },
      "source": [
        "### Install scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9A690t-cVKL"
      },
      "outputs": [],
      "source": [
        "!python -m pip install scikit-image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T2GvCMBrcQh"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "Google drive contains preprocessed DAS data.\n",
        "Data abailable upon request.  Email tculliso@stanford.edu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYwn9TmzPhIm"
      },
      "outputs": [],
      "source": [
        "# Mount my google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount a drive for saving model, patches, and compressed data\n",
        "\n",
        "This will be unique to the user"
      ],
      "metadata": {
        "id": "DbCdCZ26oUKc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCPW2jS1x9af"
      },
      "outputs": [],
      "source": [
        "# Define root path to drive\n",
        "root_fqdn = \"./drive/MyDrive/SMCityData/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K8TH-5ahzvY"
      },
      "source": [
        "### git clone custom python for project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO37TJRdf-Ae"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Haipeng-ustc/CS230-DAS-Compression.git smcitydas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copy python modules to colab local disk"
      ],
      "metadata": {
        "id": "FE11pf9pquJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltrh smcitydas"
      ],
      "metadata": {
        "id": "OeaP8fWp8Jn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybJl9-ADO622"
      },
      "outputs": [],
      "source": [
        "#!cp {root_fqdn}python/utils.py /content\n",
        "!cp smcitydas/models/*.py /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdcDa5ySpmgD"
      },
      "outputs": [],
      "source": [
        "#!cp {root_fqdn}python/model.py /content\n",
        "!ls /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fuBTElWCn64"
      },
      "source": [
        "### Import python, plotting, and tensorflow modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.experimental.numpy as tnp\n",
        "\n",
        "#TAC: moved these imports down a few cells\n",
        "\n",
        "plt.rcParams.update({'font.size': 14})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXbu9YBHK1xf"
      },
      "source": [
        "### Import Model and Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGRPhz35f5Rc"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "#TAC: above for loading new changes\n",
        "\n",
        "from model import train_das_model, make_das_codec\n",
        "from utils import extract_image_patches, extract_image_patches_fast, prepare_das_dataset\n",
        "from utils import reconstruct_image_from_patches, load_patch_from_file, norm_channel, chan_xcorr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fIaqQl2aj-K"
      },
      "source": [
        "### Creat list of path + filenames to preprocessed DAS data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNlzvBHCAp8O"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set the data for trainning\n",
        "# TAC: This will load only YOUR Google Drive.\n",
        "#file_train = sorted(glob.glob('./drive/MyDrive/DeepLearning/train/*.npz'))\n",
        "#file_test = sorted(glob.glob('./drive/MyDrive/DeepLearning/test/*.npz'))\n",
        "#file_train = sorted(glob.glob('./drive/MyDrive/SMCityData/train/smcity_sandhill*.npz')) #TAC data\n",
        "#file_test = sorted(glob.glob('./drive/MyDrive/SMCityData/test/smcity_sandhill*.npz'))   #TAC data\n",
        "file_train = sorted(glob.glob(root_fqdn+'train/*.npz')) #TAC data\n",
        "file_test = sorted(glob.glob(root_fqdn+'test/*.npz'))   #TAC data\n",
        "\n",
        "print(\"\\n--------------Trainning Dataset--------------\\n\")\n",
        "print(file_train)\n",
        "\n",
        "print(\"\\n--------------Test Dataset--------------\\n\")\n",
        "print(file_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUAOmNYxLSn_"
      },
      "source": [
        "# Prep data for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri6rx_0v_-Ia"
      },
      "source": [
        "### Set trainging data parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBu5wB5KzSF0"
      },
      "outputs": [],
      "source": [
        "# Setup parameters for generating patches\n",
        "patch_size = 32         # size of each patch\n",
        "train_ratio = 0.95      # ratio between training and dev\n",
        "shuffle = True           # shuffle all patches or not\n",
        "overlap = True          # overlap or not when retriving patches, i.e., tile for half patch_size\n",
        "add_noise = False        # add random noise to each patch or not #TAC: the model code does this already\n",
        "snr = 150                 # value for signal-to-noise ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf9YOAmTADb6"
      },
      "source": [
        "### Load data and convert to patches for traing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h05D2oSA5Ff5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Create patches from files (read all but the last file which we using for testing)\n",
        "# Should take about 1 min if reading all but last file\n",
        "# Note: output patches are labels\n",
        "input_patches, ouput_patches, factors = load_patch_from_file(file_train[:-1], patch_size, overlap = overlap, add_noise = add_noise, snr = snr)\n",
        "print('Done loading to patches')\n",
        "print(f'isize: {len(input_patches)}')\n",
        "print(f'osize: {len(ouput_patches)}')\n",
        "print(f'fsize: {len(factors)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woIHZucOicxE"
      },
      "source": [
        "### Convert patches to tf_tensors and make train and dev datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQtXRUtbi4xG"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Prepare the training and dev dataset into tensorflow\n",
        "# Includes shuffleing\n",
        "# Should take about 3 minutes when using all but last file\n",
        "train_dataset, dev_dataset = prepare_das_dataset(input_patches, ouput_patches, train_ratio = train_ratio, shuffle = shuffle)\n",
        "print('Done make datasets')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdNrU7_VilFw"
      },
      "source": [
        "### Compare an input patch to an output (label) patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRPXZScUzAiT"
      },
      "outputs": [],
      "source": [
        "# Pick a patch and compare input and label (output)\n",
        "idx = 42\n",
        "print(len(input_patches))\n",
        "\n",
        "ptitles = ['Input','Label (Output)', 'Difference']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i,data in enumerate([input_patches[idx],\n",
        "                         ouput_patches[idx],\n",
        "                         input_patches[idx] - ouput_patches[idx]]):\n",
        "\n",
        "  pclip = np.percentile(data, 99.)\n",
        "  plt.subplot(1,3,i+1)\n",
        "  plt.imshow(data, cmap='seismic', vmin=-pclip, vmax=pclip)\n",
        "  plt.title(ptitles[i])\n",
        "  plt.colorbar(orientation=\"horizontal\",fraction=0.07,anchor=(1.0,0.0))\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmUYoRiXfg5-"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "553TgjlxAacw"
      },
      "source": [
        "### Set model hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhWOHQ3aX8Sl"
      },
      "outputs": [],
      "source": [
        "# Setup parameters for training model\n",
        "lmbda = 64000         # weight for rate–distortion Lagrangian\n",
        "latent_dims = 500     # layers in latent space\n",
        "epochs = 50           # epoch\n",
        "batch_size = 2048     # batch size\n",
        "learning_rate = 1e-3  # learning rate\n",
        "validation_freq = 1   # validation frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHrLhfJYAeLg"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC03d3ESfLJA"
      },
      "outputs": [],
      "source": [
        "trainer, history = train_das_model(\n",
        "                    train_dataset,                         # training dataset\n",
        "                    dev_dataset,                           # dev dataset\n",
        "                    lmbda=lmbda,                           # weight for rate–distortion Lagrangian\n",
        "                    latent_dims=latent_dims,               # layers in latent space\n",
        "                    patch_size = patch_size,               # patch size of the image, must be dividable by 4\n",
        "                    epochs=epochs,                         # epoch\n",
        "                    batch_size=batch_size,                 # batch size\n",
        "                    learning_rate=learning_rate,           # learning rate\n",
        "                    validation_freq=validation_freq,       # validation frequency\n",
        "                    launch_trainer = True)                 # launch the trainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSWCP978Al9q"
      },
      "source": [
        "### Plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLxnZxnKobuB"
      },
      "outputs": [],
      "source": [
        "#history.history.keys()\n",
        "loss_list = ['loss', 'val_loss', 'rate_loss', 'distortion_loss']\n",
        "loss_titles = ['loss', 'dev_loss', 'rate_loss', 'distortion_loss']\n",
        "\n",
        "# Plot the rate_loss curve.\n",
        "plt.figure(figsize=(10,6))\n",
        "for i, l in enumerate(loss_list):\n",
        "  plt.subplot(2, 2, i + 1)\n",
        "  plt.plot(history.history[l])\n",
        "  plt.xlabel('#Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title(loss_titles[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('Loss.png', dpi=300, pad_inches=0.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l00M04mYapXj"
      },
      "source": [
        "## Save the model to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buwZqJ5XitMb"
      },
      "outputs": [],
      "source": [
        "# fqdn for the the model weights either for saving or reading\n",
        "#path = \"./drive/MyDrive/DeepLearning/results/\"\n",
        "path = root_fqdn+\"results/\" #TAC\n",
        "name = f\"model_patch{patch_size}_lmbda{lmbda}_latent{latent_dims}_learning_rate{learning_rate}\"\n",
        "fqdn = path+name\n",
        "print(fqdn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpu8GrUgtkMa"
      },
      "outputs": [],
      "source": [
        "# Save the model for testing\n",
        "trainer.save_weights(os.path.join(path, name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwmmQXR8E6gf"
      },
      "outputs": [],
      "source": [
        "# Verifty model was saved\n",
        "!ls {fqdn}*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqk_PhwGaumw"
      },
      "source": [
        "# Test against data not included in training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v5eCrkLAtSo"
      },
      "source": [
        "### Construct compressor and decompressor (encoder and decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIKHcYQe4r_7"
      },
      "outputs": [],
      "source": [
        "# Construct model without training\n",
        "trainer_for_test, _ = train_das_model(\n",
        "                    None,                                  # training dataset  <--- LOOK!\n",
        "                    None,                                  # dev dataset       <--- LOOK!\n",
        "                    lmbda=lmbda,                           # weight for rate–distortion Lagrangian\n",
        "                    latent_dims=latent_dims,               # layers in latent space\n",
        "                    patch_size = patch_size,               # patch size of the image, must be dividable by 4\n",
        "                    epochs=epochs,                         # epoch\n",
        "                    batch_size=batch_size,                 # batch size\n",
        "                    learning_rate=learning_rate,           # learning rate\n",
        "                    validation_freq=validation_freq,       # validation frequency\n",
        "                    launch_trainer = False)                # launch the trainer\n",
        "\n",
        "\n",
        "# load the pretrained model\n",
        "trainer_for_test.load_weights(os.path.join(path, name))\n",
        "\n",
        "# Setup the trained model\n",
        "compressor, decompressor = make_das_codec(trainer_for_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvCH__5rvdmm"
      },
      "source": [
        "## Test the reconstruction for DAS data not used for trianing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaa8YvuiBCS2"
      },
      "source": [
        "### Set indices for file used for test data and one for non-similar data\n",
        "The non-similar data has the same channels but different time to compare with cross-correlation results shown near the bottomw of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpK0ifdQxICx"
      },
      "outputs": [],
      "source": [
        "# which file for testing (last file not used for training)\n",
        "test_idx = 6 #TAC: last file\n",
        "\n",
        "# which file for x-correlation of dissimilar images (negative control)\n",
        "xcor_idx = 5 #TAC: second to last file (so same geographic region but different time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_88ztCNEBNgO"
      },
      "source": [
        "### Conver test data and non-similar data into patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q2AJ37DtsnS"
      },
      "outputs": [],
      "source": [
        "# Load the one of the test data\n",
        "pdata = norm_channel(np.load(file_test[test_idx])['pdata'])\n",
        "patches_original, _toss, factors = extract_image_patches(pdata, patch_size, add_noise=False)\n",
        "patches_original = tf.convert_to_tensor(patches_original, dtype=tf.float32)\n",
        "\n",
        "# These data are used for showing x-correlation between two dissimular data sets (like a negative-control)\n",
        "xcor_pdata = norm_channel(np.load(file_test[xcor_idx])['pdata'])\n",
        "xcor_patches, _toss, xfactors = extract_image_patches(xcor_pdata, patch_size, add_noise=False)\n",
        "xcor_patches = tf.convert_to_tensor(xcor_patches, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz2Fs1qExdky"
      },
      "source": [
        "### Compress the patches for the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBweLBawqL4j"
      },
      "outputs": [],
      "source": [
        "# Compress the patches to strings, and keep track of each of their information content in bits.\n",
        "strings, entropies = compressor(patches_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV8WJSxdjtMf"
      },
      "source": [
        "### Show info regarding size of compressed data and the entropy bits.\n",
        "\n",
        "**When patch_size = 32** we get a ~30X compression rate\n",
        "(Note: when stored to disk, the numpy array object fields take up more bytes than the compressed data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2fe9htpwTTh"
      },
      "outputs": [],
      "source": [
        "from sys import getsizeof\n",
        "print(f'patch_size: {patch_size}')\n",
        "\n",
        "len_ptc = len(patches_original)\n",
        "len_str = len(strings)\n",
        "len_ent = len(entropies)\n",
        "np_strings = strings.numpy()\n",
        "print(f'len(patches): {len_ptc}')\n",
        "print(f'len(strings): {len_str}')\n",
        "print(f'strings.nelement(): {strings.dtype}')\n",
        "print(f'len(entropies): {len_ent}')\n",
        "print()\n",
        "print(f'strings.shape: {(strings.shape)}')\n",
        "print(f'strings.dtype: {(strings.dtype)}')\n",
        "print(f'np_strings: {(np_strings.nbytes)}')\n",
        "print(f'np_strings.bytes: {getsizeof(strings[0])/8}')\n",
        "\n",
        "str_bytes = sum([getsizeof(nb) for nb in np_strings])\n",
        "ptc_bytes = len_ptc*4*patch_size**2\n",
        "comp_rat = ptc_bytes/str_bytes\n",
        "print(f'str_bytes: {str_bytes}')\n",
        "print(f'ptc_bytes: {ptc_bytes}')\n",
        "print(f'approximate compression rate: {comp_rat}')\n",
        "print()\n",
        "print(f'entropies.max: {entropies.numpy().max()}')\n",
        "print(f'entropies.min: {entropies.numpy().min()}')\n",
        "print(f'entropies.mean: {entropies.numpy().mean()}')\n",
        "print(f'entropies.sum: {np.sum(entropies.numpy())/8}')\n",
        "\n",
        "ibytes = 0\n",
        "max_bytes = 0\n",
        "for i in range(len_str):\n",
        "  tmp = getsizeof(np_strings[i])\n",
        "  ibytes += tmp\n",
        "  if max_bytes < tmp:\n",
        "    max_bytes = tmp\n",
        "  #print(f'strings[{i}]: {ibytes}')\n",
        "print(f'total strings.bytes: {ibytes}')\n",
        "print(f'max strings.bytes: {max_bytes}')\n",
        "\n",
        "print(getsizeof(['abc']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approximate compression rate\n",
        "\n",
        "Note, this can change based on hyperparameters and the dataset compressed. On average, we get approximately a 30X compression rate\n",
        "\n",
        "**See output below**"
      ],
      "metadata": {
        "id": "QgTdg1G5VanF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'approximate compression rate: {comp_rat}')"
      ],
      "metadata": {
        "id": "Pd4DVqsmVZbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBTjAJmgEBAg"
      },
      "source": [
        "# Analyze compression results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR6WJdw0xuTf"
      },
      "source": [
        "### Decompress the patches from the strings back to data patches (float32 values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVBQN-IdwPKQ"
      },
      "outputs": [],
      "source": [
        "patches_decompre = decompressor(strings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNlPTZFPxxhV"
      },
      "source": [
        "### Plot patch comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWCoSKpWuBHn"
      },
      "outputs": [],
      "source": [
        "#patch index\n",
        "idx = 42\n",
        "\n",
        "ptitles=['Original','Decomp','Difference']\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(patches_original[idx])\n",
        "plt.title(ptitles[0])\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(patches_decompre[idx])\n",
        "plt.title(ptitles[1])\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(patches_original[idx] - patches_decompre[idx])\n",
        "plt.title(ptitles[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7tg2E3Cw_7g"
      },
      "source": [
        "### Reconstruct full test dataset from decompressed patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cairph8PqQH-"
      },
      "outputs": [],
      "source": [
        "# Load the original data just from one file\n",
        "original_shape = pdata.shape\n",
        "print('shape of original data: ', original_shape)\n",
        "\n",
        "original_das = reconstruct_image_from_patches(patches_original, factors, original_shape, patch_size).astype(np.float32)\n",
        "decompre_das = reconstruct_image_from_patches(patches_decompre, factors, original_shape, patch_size).astype(np.float32)\n",
        "xcor_das = reconstruct_image_from_patches(xcor_patches, xfactors, original_shape, patch_size).astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGfSi6hNahQJ"
      },
      "source": [
        "### Save data and compressed patches to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmEtd9LY96f6"
      },
      "outputs": [],
      "source": [
        "# # Save the compressed data for stand-alone test\n",
        "# np.save('./drive/MyDrive/Training Dataset/original_das', original_das)\n",
        "# np.save('./drive/MyDrive/Training Dataset/decompre_das', decompre_das)\n",
        "# np.save('./drive/MyDrive/Training Dataset/strings.npy', strings.numpy())\n",
        "# np.save('./drive/MyDrive/Training Dataset/entropies.npy', entropies.numpy())\n",
        "# np.save('./drive/MyDrive/Training Dataset/patches_original.npy', patches_original.numpy())\n",
        "# np.save('./drive/MyDrive/Training Dataset/factors.npy', factors)\n",
        "\n",
        "do_save = False\n",
        "\n",
        "if do_save:\n",
        "  tpath = root_fqdn+'results4test/' #TAC\n",
        "  np.save(tpath+'original_das_idx-{test_idx}', original_das)\n",
        "  np.save(tpath+'decompre_das_idx-{test_idx}', decompre_das)\n",
        "  np.save(tpath+'strings_idx-{test_idx}.npy', strings.numpy())\n",
        "  np.save(tpath+'entropies_idx-{test_idx}.npy', entropies.numpy())\n",
        "  np.save(tpath+'patches_original_idx-{test_idx}.npy', patches_original.numpy())\n",
        "  np.save(tpath+'factors_idx-{test_idx}.npy', factors)\n",
        "\n",
        "  # TAC: doesn't make much of a difference\n",
        "  #np.savez_compressed(tpath+'strings.npz', strings=strings.numpy())\n",
        "  #np.savez_compressed(tpath+'patches_original.npz', patches_original=patches_original.numpy())\n",
        "  #np.savez_compressed('factors.npz', factors=factors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kja6Fvz3Mgi-"
      },
      "source": [
        "### Read data for strings (if desired). Only needed for testing from disk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_T9vmN4PYERD"
      },
      "outputs": [],
      "source": [
        "#tpath = root_fqdn+'results4test/' #TAC\n",
        "#l_np_original_das = np.load(tpath+'original_das_idx-{test_idx}.npy')\n",
        "#l_np_decompre_das = np.load(tpath+'decompre_das_idx-{test_idx}.npy')\n",
        "#l_np_strings      = np.load(tpath+'strings_idx-{test_idx}.npy',allow_pickle=True)\n",
        "#l_np_entropies    = np.load(tpath+'entropies_idx-{test_idx}.npy')\n",
        "#l_np_patches_original = np.load(tpath+'patches_original_idx-{test_idx}.npy')\n",
        "#l_np_factors = np.load(tpath+'factors_idx-{test_idx}.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI_NWQA5CPgB"
      },
      "source": [
        "### Run this cell only if you want to test reading the compressed data from disk and then reconstructing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKI5WKvpi1HH"
      },
      "outputs": [],
      "source": [
        "#strings = tf.convert_to_tensor(l_np_strings)\n",
        "#entropies = tf.convert_to_tensor(l_np_entropies)\n",
        "\n",
        "# load normlazation factor\n",
        "#factors = l_np_factors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-l94nD6yCQ7"
      },
      "source": [
        "### Plot and compare the full test dataste for the original, decompressed, and difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s8w9uept0mQ"
      },
      "outputs": [],
      "source": [
        "# plot\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "nc = original_shape[0]\n",
        "rmch = nc%patch_size\n",
        "nc -= rmch\n",
        "\n",
        "s_nc = 0\n",
        "s_nt = 0\n",
        "# nc = 3000\n",
        "nt = 900\n",
        "aspect = 'auto'\n",
        "vclip = 0.9\n",
        "diff_gain = 0.1\n",
        "\n",
        "plt.subplot(311)\n",
        "orig_plt = original_das[s_nc:nc, s_nt:nt] #TAC\n",
        "orig_max = np.abs(orig_plt).max()\n",
        "pclip = (1.-vclip)*orig_max\n",
        "im = plt.imshow(orig_plt, cmap='gray', aspect=aspect, vmin=-pclip, vmax=pclip) #TAC\n",
        "# plt.xlabel('#Time samples')\n",
        "plt.title('Original DAS Data')\n",
        "plt.ylabel('Channels')\n",
        "cbar = plt.colorbar(im, shrink=0.99, pad=0.01)\n",
        "\n",
        "plt.subplot(312)\n",
        "decom_plt = decompre_das[s_nc:nc, s_nt:nt] #TAC\n",
        "im = plt.imshow(decom_plt, cmap='gray', aspect=aspect, vmin=-pclip, vmax=pclip) #TAC\n",
        "# plt.xlabel('#Time samples')\n",
        "plt.title('Decompressed DAS Data')\n",
        "plt.ylabel('Channels')\n",
        "cbar = plt.colorbar(im, shrink=0.99, pad=0.01)\n",
        "\n",
        "plt.subplot(313)\n",
        "diff_plt = orig_plt - decom_plt #TAC\n",
        "im = plt.imshow(diff_plt, cmap='gray', aspect=aspect, vmin=-diff_gain*pclip, vmax=diff_gain*pclip) #TAC\n",
        "plt.xlabel('Time samples')\n",
        "plt.title(f'Difference (with {int(1/diff_gain)}X gain)')\n",
        "plt.ylabel('Channels')\n",
        "cbar = plt.colorbar(im, shrink=0.99, pad=0.01)\n",
        "\n",
        "plt.tight_layout()\n",
        "fname = f'results/DAS_final_patch{patch_size}_noise{add_noise}_lmbda{lmbda}_latent_dims{latent_dims}_test{i}.png'\n",
        "plt.savefig(root_fqdn+fname, dpi=300, pad_inches=0.1) #TAC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO9_nFgo0NPt"
      },
      "source": [
        "## Calculate X-correlations (Set 'xcorr_axis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAJMLggZEWzu"
      },
      "source": [
        "### Compute the cross-correlations\n",
        "\n",
        "* original with original (auto correlation)\n",
        "* original with decompressed\n",
        "* difference between cross-correlations above\n",
        "* original with non-similar data (data at a different time)\n",
        "\n",
        "**For time cross-correlations:**    set *xcorr_axis* = -1\n",
        "<br>\n",
        "**For channel cross-correlations:** set *xcorr_axis* = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbJPCac_rcI3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# set axis to x-correlate (0 or -1) Only!\n",
        "xcorr_axis = 0\n",
        "\n",
        "assert (xcorr_axis == 0) or (xcorr_axis == -1)\n",
        "\n",
        "stack_axis = 0\n",
        "if xcorr_axis == 0:\n",
        "  stack_axis = -1\n",
        "\n",
        "\n",
        "# slice unsimilarity data consistant with original and decomp data\n",
        "xcor_plt = xcor_das[s_nc:nc, s_nt:nt] #NOTE: slice parms defined in cell above\n",
        "\n",
        "\n",
        "# calculate the x-correlations\n",
        "auto_xcorr = chan_xcorr(orig_plt,orig_plt,which_axis=xcorr_axis)\n",
        "decomp_xcorr = chan_xcorr(orig_plt,decom_plt,which_axis=xcorr_axis)\n",
        "diff_xcorr = auto_xcorr - decomp_xcorr\n",
        "nosim_xcorr = chan_xcorr(orig_plt,xcor_plt,which_axis=xcorr_axis)\n",
        "\n",
        "print(f'max diff: {diff_xcorr.max()}')\n",
        "print(f'min diff: {diff_xcorr.min()}')\n",
        "\n",
        "# Find the indices for the max of the x-correlations\n",
        "imax_auto = np.argmax(auto_xcorr,axis=xcorr_axis)\n",
        "imax_decomp = np.argmax(decomp_xcorr,axis=xcorr_axis)\n",
        "imax_diff = np.argmax(diff_xcorr,axis=xcorr_axis)\n",
        "imax_nosim = np.argmax(nosim_xcorr,axis=xcorr_axis)\n",
        "\n",
        "# In the best case diff_argmax should be a vector of zeros\n",
        "diff_argmax = imax_auto - imax_decomp\n",
        "print(f'sum(argmax_diff): {np.sum(diff_argmax)}')\n",
        "for i in range(len(diff_argmax)):\n",
        "  if diff_argmax[i] != 0:\n",
        "    print(f'xcorr diff = diff_argmax[{i}] = {diff_argmax[i]}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8rBYXCt0TMH"
      },
      "source": [
        "### Plot X-correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGJu7M9Qt0mS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.ticker as ticker\n",
        "ptitles = ['Orig X Orig','Orig X Decomp','Diff','Orig X Not-Similar']\n",
        "\n",
        "# Set the figure size and font size\n",
        "fig = plt.figure(figsize=(10, 4.8))\n",
        "plt.rcParams.update({'font.size': 10})\n",
        "\n",
        "# Plot the subplots\n",
        "for i, data in enumerate([auto_xcorr, decomp_xcorr, diff_xcorr, nosim_xcorr]):\n",
        "    ax = fig.add_subplot(1, 4, i+1)\n",
        "    im = plt.imshow(data,cmap='gray',aspect='auto',vmin=-1,vmax=1)\n",
        "    # labels\n",
        "    if xcorr_axis == -1:\n",
        "      plt.xlabel('Time Lag')\n",
        "    else:\n",
        "      plt.xlabel('Time Samples')\n",
        "    if i==0:\n",
        "      if xcorr_axis == -1:\n",
        "        plt.ylabel('Channels')\n",
        "      else:\n",
        "        plt.ylabel('Channel Lag')\n",
        "    else:\n",
        "        ax.set_yticklabels([])\n",
        "\n",
        "    if xcorr_axis == -1:\n",
        "      #x ticks\n",
        "      ax.set_xticks(np.linspace(0, 9, 5) * 100)\n",
        "      ax.set_xticklabels(np.linspace(0, 9, 5) * 100 - 450)\n",
        "    else:\n",
        "      #x ticks\n",
        "      ax.set_xticks(np.arange(6)*180)\n",
        "      ax.set_xticklabels(np.arange(6) * 180)\n",
        "\n",
        "      #y ticks\n",
        "      ax.set_yticks(np.linspace(0, nc/100, 7) * 100)\n",
        "      yticks = np.linspace(0, nc/100, 7) * 100 - nc/2\n",
        "      ax.set_yticklabels(yticks.astype(np.int32))\n",
        "\n",
        "    plt.title(ptitles[i])\n",
        "\n",
        "    # colorbar\n",
        "    cbar = fig.colorbar(im, orientation=\"horizontal\", shrink=0.8, pad=0.12)\n",
        "\n",
        "    # Title, if desired\n",
        "    #plt.title('Reconstructed DAS Data')\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "fig.tight_layout()\n",
        "fname = 'correlation_time.png'\n",
        "if xcorr_axis != -1:\n",
        "  fname = 'correlation_channel.png'\n",
        "plt.savefig(root_fqdn+'results/'+fname, dpi=300)\n",
        "print(root_fqdn+'results/'+fname)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxHbrAlv8FZz"
      },
      "source": [
        "## Histograms of the difference and the Structural Similarity Measure between Original and Decompressed Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdGqJZp4FM9J"
      },
      "source": [
        "### Compute structural similartiy measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFnWinf8t0mX"
      },
      "outputs": [],
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# Structural similarity index\n",
        "SSIM = []\n",
        "for y, y_hat in zip(patches_original, patches_decompre):\n",
        "    y1 = y.numpy().squeeze()\n",
        "    y2 = y_hat.numpy().squeeze()\n",
        "    SSIM.append(ssim(y1,y2, data_range=y1.max() - y1.min()))\n",
        "SSIM = np.array(SSIM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJngeKa_FTi5"
      },
      "source": [
        "### Plot histograms (there are two)\n",
        "\n",
        "* One for the values of the difference between the original data and the decompressed data\n",
        "* One for the structural similarity for each patch from the original data compared to the decompressed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1ZsJMjst0mY"
      },
      "outputs": [],
      "source": [
        "# Create a figure and axis\n",
        "fig, ax = plt.subplots(1,2,figsize=(8, 3.5))\n",
        "\n",
        "# Create the histogram\n",
        "n, bins, patches = ax[0].hist(diff_plt.flatten(), bins=300, edgecolor='black', alpha=0.75, color='#FF0000')\n",
        "\n",
        "# Customize the appearance\n",
        "#ax[0].set_xlabel('', fontsize=12)\n",
        "ax[0].set_ylabel('Number of Data Points', fontsize=12)\n",
        "ax[0].set_xlabel('Difference Value', fontsize=14)\n",
        "\n",
        "ax[0].grid(axis='y', alpha=0.5)\n",
        "ax[0].spines['top'].set_visible(False)\n",
        "ax[0].spines['right'].set_visible(False)\n",
        "\n",
        "# Customize the tick labels\n",
        "ax[0].tick_params(axis='both', which='major', labelsize=10)\n",
        "ax[0].tick_params(axis='both', which='minor', labelsize=8)\n",
        "ax[0].set_xlim(-12, 12)\n",
        "\n",
        "# Create the histogram\n",
        "n, bins, patches = ax[1].hist(SSIM.flatten(), bins=16, edgecolor='black', alpha=0.75, color='#87CEEB')\n",
        "\n",
        "# Customize the appearance\n",
        "ax[1].set_xlabel('Structural Similarity', fontsize=12)\n",
        "ax[1].set_ylabel('Number of Patches', fontsize=12)\n",
        "# ax[1].set_title('Structural Similarity', fontsize=14)\n",
        "\n",
        "ax[1].grid(axis='y', alpha=0.5)\n",
        "ax[1].spines['top'].set_visible(False)\n",
        "ax[1].spines['right'].set_visible(False)\n",
        "\n",
        "# Customize the tick labels\n",
        "ax[1].tick_params(axis='both', which='major', labelsize=10)\n",
        "ax[1].tick_params(axis='both', which='minor', labelsize=8)\n",
        "ax[1].set_xlim(0.5, 1)\n",
        "\n",
        "# Save the figure\n",
        "plt.tight_layout()\n",
        "plt.savefig(root_fqdn+'results/histograms.png', dpi=300)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV9oSDLI9O4Z"
      },
      "source": [
        "## Compare frequency spectrums in f-k domain (frequency-wavenumber)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeAoOF07t0mY"
      },
      "outputs": [],
      "source": [
        "# Define function for computing specturm and plotting\n",
        "#def fk_spectrum(signal, time_axis, x_axis, vclip = 0.7, f_lim = None, kx_lim = None, ptitle=''):\n",
        "def fk_spectrum(signal, time_axis, x_axis, vclip = 0.7, f_lim = None, kx_lim = None, ptitle='',clip=-1,c_spec=None):\n",
        "\n",
        "    # [time axis, spatial axis]\n",
        "    signal = signal.T\n",
        "    Nt, Nx = signal.shape\n",
        "\n",
        "    if Nt != time_axis.shape[0] or Nx != x_axis.shape[0]:\n",
        "        raise ValueError(\"Wrong size\")\n",
        "\n",
        "    # Perform 2D Fourier Transform\n",
        "    spectrum = np.fft.fftshift(np.fft.fft2(signal))\n",
        "\n",
        "    #plot the difference between compute specturm another spectrum\n",
        "    if c_spec is not None:\n",
        "      spectrum -= c_spec\n",
        "\n",
        "    # Generate frequency and wavenumber axes\n",
        "    dt = time_axis[1] - time_axis[0]\n",
        "    dx = x_axis[1] - x_axis[0]\n",
        "    freq_axis = np.fft.fftshift(np.fft.fftfreq(Nt, dt))\n",
        "    wavenum_axis = np.fft.fftshift(np.fft.fftfreq(Nx, dx))\n",
        "\n",
        "    # Plot F-K spectrum\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    pclip = (1.-vclip)*np.abs(spectrum).max()\n",
        "    if clip != -1:\n",
        "      pclip = clip\n",
        "    plt.imshow(np.abs(spectrum), vmin=0, vmax=pclip, aspect='auto',\n",
        "               extent=[wavenum_axis.min(), wavenum_axis.max(),freq_axis.min(), freq_axis.max()])\n",
        "\n",
        "    plt.xlabel('Wavenumber')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.colorbar(label='Amplitude')\n",
        "    plt.title('F-K Spectrum: '+ptitle)\n",
        "#     if kx_lim is not None:\n",
        "#         plt.xlim(kx_lim)\n",
        "#     if f_lim is not None:\n",
        "#         plt.ylim(f_lim)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return spectrum, freq_axis, wavenum_axis, pclip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LedIEBFxt0mZ"
      },
      "outputs": [],
      "source": [
        "Nx, Nt = original_das.shape\n",
        "dt = 1/5    # in seconds\n",
        "dx = 1     # in meter (Not sure!!!!!)\n",
        "\n",
        "# Generate synthetic signal\n",
        "time_axis = np.linspace(0, (nt-1) * dt, nt)\n",
        "x_axis    = np.linspace(0, (nc-1) * dx, nc)\n",
        "\n",
        "diff_gain = 0.1\n",
        "\n",
        "# Perform F-K spectrum analysis\n",
        "orig_spec, freq_axis, wavenum_axis, pclip = fk_spectrum(orig_plt, time_axis, x_axis, vclip = 0.7, f_lim=[-1, 1], kx_lim=[-0.02, 0.02],ptitle='Original')\n",
        "\n",
        "decomp_spec, freq_axis, wavenum_axis, junk = fk_spectrum(decom_plt, time_axis, x_axis, vclip = 0.7, f_lim=[-1, 1], kx_lim=[-0.02, 0.02],ptitle='Decompressed',clip=pclip)\n",
        "\n",
        "diff_spec, freq_axis, wavenum_axis, junk = fk_spectrum(diff_plt, time_axis, x_axis, vclip = 0.7, f_lim=[-1, 1], kx_lim=[-0.02, 0.02],ptitle='Difference (with 10X gain)',clip=diff_gain*pclip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_zX4wUUjQtl"
      },
      "source": [
        "# Show Model Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LecCwL42VPon"
      },
      "outputs": [],
      "source": [
        "# show the encoder model\n",
        "trainer.analysis_transform.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb7Nw79ViVe6"
      },
      "outputs": [],
      "source": [
        "# show the decoder model\n",
        "trainer.synthesis_transform.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoQQcOepiUgA"
      },
      "outputs": [],
      "source": [
        "# plot the encoder model\n",
        "tf.keras.utils.plot_model(trainer.analysis_transform, to_file='model_analysis_transform.png', show_shapes=True, show_dtype=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdHx6vWmiUGg"
      },
      "outputs": [],
      "source": [
        "# plot the decoder model\n",
        "tf.keras.utils.plot_model(trainer.synthesis_transform, to_file='model_synthesis_transform.png', show_shapes=True, show_dtype=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VteuooQmyUx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}